"""
{{ project_name }} Data Pipeline
==================================
Production-ready orchestration DAG with:
- Isolated execution via DockerOperator
- Comprehensive error handling and retries
- Slack/Email notifications
- Data quality validation
- Task grouping for better organization
"""

from airflow import DAG
from airflow.providers.docker.operators.docker import DockerOperator
from airflow.operators.python import PythonOperator
from airflow.utils.task_group import TaskGroup
from airflow.utils.dates import days_ago
from datetime import datetime, timedelta
import os
import logging

logger = logging.getLogger(__name__)

# =============================================================================
# CONFIGURATION
# =============================================================================

# Default arguments with production-ready retry logic
default_args = {
    'owner': 'data-engineering',
    'depends_on_past': False,
    'email': os.getenv('ALERT_EMAIL', 'admin@example.com').split(','),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'retry_exponential_backoff': True,
    'max_retry_delay': timedelta(minutes=30),
    'execution_timeout': timedelta(hours=2),
}

# Slack notification callback (if webhook configured)
def slack_failure_callback(context):
    """Send Slack notification on task failure"""
    try:
        from airflow.providers.slack.hooks.slack_webhook import SlackWebhookHook
        
        slack_webhook_url = os.getenv('SLACK_WEBHOOK_URL')
        if not slack_webhook_url:
            logger.warning("SLACK_WEBHOOK_URL not configured, skipping Slack notification")
            return
        
        slack_hook = SlackWebhookHook(
            http_conn_id='slack_webhook',
            webhook_token=slack_webhook_url,
            message=f"""
:red_circle: *Task Failed*
*DAG*: {context.get('task_instance').dag_id}
*Task*: {context.get('task_instance').task_id}
*Execution Time*: {context.get('execution_date')}
*Log URL*: {context.get('task_instance').log_url}
            """,
            username='Airflow Bot'
        )
        slack_hook.execute()
    except Exception as e:
        logger.error(f"Failed to send Slack notification: {e}")

# Add callback to default_args if Slack is configured
if os.getenv('SLACK_WEBHOOK_URL'):
    default_args['on_failure_callback'] = slack_failure_callback

# =============================================================================
# DAG DEFINITION
# =============================================================================

with DAG(
    dag_id='{{ project_name }}_pipeline',
    default_args=default_args,
    description='Enterprise data pipeline for {{ project_name }}',
    schedule_interval='0 2 * * *',  # Daily at 2 AM UTC
    start_date=days_ago(1),
    catchup=False,
    max_active_runs=1,
    tags=['production', '{{ project_name }}', 'data-pipeline'],
    doc_md=__doc__,
) as dag:

    # =========================================================================
    # PRE-FLIGHT CHECKS
    # =========================================================================
    
    def validate_environment(**context):
        """Validate required environment variables"""
        required_vars = [
            'DB_HOST', 'DB_PORT', 'DB_NAME', 'DB_USER', 'DB_PASSWORD'
        ]
        missing = [var for var in required_vars if not os.getenv(var)]
        
        if missing:
            raise ValueError(f"Missing required environment variables: {', '.join(missing)}")
        
        logger.info("✓ Environment validation passed")
        return True
    
    preflight_check = PythonOperator(
        task_id='preflight_check',
        python_callable=validate_environment,
        doc_md="Validates environment configuration before pipeline execution"
    )

    # =========================================================================
    # INGESTION TASK GROUP
    # =========================================================================
    
    with TaskGroup(group_id='ingestion', tooltip='Data ingestion tasks') as ingestion_group:
        
        {% if stack.ingestion == "DLT" %}
        run_dlt_pipeline = DockerOperator(
            task_id='run_dlt_extraction',
            image='python:3.11-slim',
            api_version='auto',
            auto_remove=True,
            command='/bin/bash -c "pip install dlt && python /app/pipelines/dlt_pipeline.py"',
            docker_url='unix://var/run/docker.sock',
            network_mode='antigravity_net',
            mount_tmp_dir=False,
            mounts=[
                {
                    'source': '{{ project_name }}_dlt_data',
                    'target': '/app/.dlt',
                    'type': 'volume'
                }
            ],
            environment={
                'DB_HOST': os.getenv('DB_HOST'),
                'DB_PORT': os.getenv('DB_PORT'),
                'DB_NAME': os.getenv('DB_NAME'),
                'DB_USER': os.getenv('DB_USER'),
                'DB_PASSWORD': os.getenv('DB_PASSWORD'),
            },
            doc_md="Extracts data using DLT framework with isolated execution"
        )
        {% endif %}
        
        def validate_ingestion_quality(**context):
            """Validate ingested data quality"""
            import psycopg2
            
            conn = psycopg2.connect(
                host=os.getenv('DB_HOST'),
                port=os.getenv('DB_PORT'),
                dbname=os.getenv('DB_NAME'),
                user=os.getenv('DB_USER'),
                password=os.getenv('DB_PASSWORD')
            )
            
            cursor = conn.cursor()
            
            # Example: Check row count in raw tables
            cursor.execute("SELECT COUNT(*) FROM raw_data_table")  # Adjust table name
            row_count = cursor.fetchone()[0]
            
            if row_count == 0:
                raise ValueError("No data ingested - pipeline aborted")
            
            logger.info(f"✓ Ingestion quality check passed: {row_count} rows ingested")
            
            cursor.close()
            conn.close()
            return row_count
        
        quality_check_ingestion = PythonOperator(
            task_id='quality_check',
            python_callable=validate_ingestion_quality,
            doc_md="Validates data quality after ingestion"
        )

    # =========================================================================
    # TRANSFORMATION TASK GROUP
    # =========================================================================
    
    with TaskGroup(group_id='transformation', tooltip='dbt transformation tasks') as transformation_group:
        
        {% if stack.transformation == "dbt" %}
        # dbt deps: Install dependencies
        dbt_deps = DockerOperator(
            task_id='dbt_deps',
            image='ghcr.io/dbt-labs/dbt-{{ stack.storage|lower if stack.storage else "postgres" }}:latest',
            api_version='auto',
            auto_remove=True,
            command='dbt deps --profiles-dir /app',
            docker_url='unix://var/run/docker.sock',
            network_mode='antigravity_net',
            mounts=[
                {
                    'source': '{{ project_name }}_dbt_project',
                    'target': '/app',
                    'type': 'volume'
                }
            ],
            environment={
                'DBT_TARGET': os.getenv('DBT_TARGET', 'dev'),
                'DB_HOST': os.getenv('DB_HOST'),
                'DB_PORT': os.getenv('DB_PORT'),
                'DB_NAME': os.getenv('DB_NAME'),
                'DB_USER': os.getenv('DB_USER'),
                'DB_PASSWORD': os.getenv('DB_PASSWORD'),
            },
        )
        
        # dbt run: Execute models
        dbt_run = DockerOperator(
            task_id='dbt_run',
            image='ghcr.io/dbt-labs/dbt-{{ stack.storage|lower if stack.storage else "postgres" }}:latest',
            api_version='auto',
            auto_remove=True,
            command='dbt run --profiles-dir /app --full-refresh',
            docker_url='unix://var/run/docker.sock',
            network_mode='antigravity_net',
            mounts=[
                {
                    'source': '{{ project_name }}_dbt_project',
                    'target': '/app',
                    'type': 'volume'
                }
            ],
            environment={
                'DBT_TARGET': os.getenv('DBT_TARGET', 'dev'),
                'DB_HOST': os.getenv('DB_HOST'),
                'DB_PORT': os.getenv('DB_PORT'),
                'DB_NAME': os.getenv('DB_NAME'),
                'DB_USER': os.getenv('DB_USER'),
                'DB_PASSWORD': os.getenv('DB_PASSWORD'),
            },
            doc_md="Runs dbt models to transform raw data"
        )
        
        # dbt test: Run data quality tests
        dbt_test = DockerOperator(
            task_id='dbt_test',
            image='ghcr.io/dbt-labs/dbt-{{ stack.storage|lower if stack.storage else "postgres" }}:latest',
            api_version='auto',
            auto_remove=True,
            command='dbt test --profiles-dir /app',
            docker_url='unix://var/run/docker.sock',
            network_mode='antigravity_net',
            mounts=[
                {
                    'source': '{{ project_name }}_dbt_project',
                    'target': '/app',
                    'type': 'volume'
                }
            ],
            environment={
                'DBT_TARGET': os.getenv('DBT_TARGET', 'dev'),
                'DB_HOST': os.getenv('DB_HOST'),
                'DB_PORT': os.getenv('DB_PORT'),
                'DB_NAME': os.getenv('DB_NAME'),
                'DB_USER': os.getenv('DB_USER'),
                'DB_PASSWORD': os.getenv('DB_PASSWORD'),
            },
            doc_md="Runs dbt tests to ensure data quality"
        )
        
        dbt_deps >> dbt_run >> dbt_test
        {% endif %}

    # =========================================================================
    # POST-PIPELINE TASKS
    # =========================================================================
    
    def send_success_notification(**context):
        """Send success notification"""
        execution_date = context['execution_date']
        logger.info(f"✓ Pipeline completed successfully for {execution_date}")
        
        # Could send Slack success message here
        return True
    
    success_notification = PythonOperator(
        task_id='success_notification',
        python_callable=send_success_notification,
        doc_md="Sends success notification after pipeline completion"
    )

    # =========================================================================
    # TASK DEPENDENCIES
    # =========================================================================
    
    preflight_check >> ingestion_group >> transformation_group >> success_notification

