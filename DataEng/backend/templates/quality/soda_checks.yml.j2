# Soda Data Quality Checks
# Auto-generated by AntiGravity for {{ project_name }}

# Connection configuration
data_source {{ project_name }}_warehouse:
  type: {{ stack.storage | lower }}
{% if stack.storage == 'PostgreSQL' %}
  host: postgres
  port: 5432
  username: {{ secrets.postgres_user }}
  password: {{ secrets.postgres_password }}
  database: {{ project_name }}_warehouse
  schema: public
{% elif stack.storage == 'Snowflake' %}
  account: {{ secrets.snowflake_account }}
  username: {{ secrets.snowflake_user }}
  password: {{ secrets.snowflake_password }}
  database: {{ project_name }}_warehouse
  warehouse: {{ secrets.snowflake_warehouse }}
  role: {{ secrets.snowflake_role | default('SYSADMIN') }}
  schema: PUBLIC
{% elif stack.storage == 'BigQuery' %}
  account_info_json_path: /app/credentials/bigquery-key.json
  project_id: {{ secrets.bigquery_project }}
  dataset: {{ project_name }}_dataset
{% elif stack.storage == 'Redshift' %}
  host: {{ secrets.redshift_host }}
  port: 5439
  username: {{ secrets.redshift_user }}
  password: {{ secrets.redshift_password }}
  database: {{ project_name }}_warehouse
  schema: public
{% endif %}

# =============================================================================
# TABLE-LEVEL CHECKS
# =============================================================================

checks for sample_table:  # Replace with your actual table name
  
  # Row count checks
  - row_count > 0
  - row_count:
      warn: when < 100
      fail: when < 10
  
  # Freshness checks (for tables with timestamp columns)
  - max_freshness(created_at) < 24h
  
  # Duplicate checks
  - duplicate_count(id) = 0
  
  # Schema evolution detection
  - schema:
      warn:
        when schema changes:
          - column delete
          - type change
      fail:
        when wrong column type:
          id: integer
          created_at: timestamp

# =============================================================================
# COLUMN-LEVEL CHECKS
# =============================================================================

checks for sample_table:
  
  # NULL checks
  - missing_count(id) = 0
  - missing_count(created_at) = 0
  - missing_percent(email):
      warn: when > 5%
      fail: when > 10%
  
  # Uniqueness
  - duplicate_count(id) = 0
  - unique_count(id) = row_count
  
  # Value ranges
  - min(age) >= 0
  - max(age) <= 120
  - avg(age):
      warn: when < 18 or > 100
  
  # Categorical values
  - invalid_count(status) = 0:
      valid values: ['active', 'inactive', 'pending']
  
  # String patterns
  - invalid_percent(email) = 0:
      valid regex: '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
  
  # Referential integrity
  - values in (user_id) must exist in users (id)
  
  # Statistics
  - stddev(price):
      warn: when > 1000
  
  # Custom SQL checks
  - failed rows:
      sql: |
        SELECT *
        FROM sample_table
        WHERE created_at > updated_at
      fail condition: count > 0

# =============================================================================
# CROSS-TABLE CHECKS
# =============================================================================

checks for sample_table:
  # Ensure related tables have matching records
  - failed rows:
      sql: |
        SELECT st.*
        FROM sample_table st
        LEFT JOIN users u ON st.user_id = u.id
        WHERE u.id IS NULL
      fail condition: count > 0

# =============================================================================
# ANOMALY DETECTION
# =============================================================================

checks for sample_table:
  # Detect anomalies in numeric columns using historical data
  - anomaly_score(revenue) < 3:
      anomaly_detector: prophet
  
  # Detect distribution changes
  - distribution_difference(price) < 0.1:
      method: ks_test
      baseline: last 7 days

# =============================================================================
# DATA PROFILING
# =============================================================================

profile columns:
  columns:
    - id
    - created_at
    - email
    - status
  limit: 10000
  sample_method: random

# =============================================================================
# ALERT CONFIGURATION
# =============================================================================

alert_on_fail: true
alert_channels:
  - type: slack
    webhook_url: ${SODA_SLACK_WEBHOOK}
  - type: email
    recipients:
      - data-team@{{ project_name }}.com

# =============================================================================
# SCHEDULING HINTS
# =============================================================================

# Run these checks:
# - After each dbt run (via dbt test or Airflow)
# - On a schedule (e.g., hourly for critical tables)
# - Before deploying to production

# Example Airflow integration:
# soda_scan = BashOperator(
#     task_id='soda_quality_checks',
#     bash_command='soda scan -d {{ project_name }}_warehouse -c soda_checks.yml'
# )
